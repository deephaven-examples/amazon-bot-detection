{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2739477-43f9-4186-9340-0ed74fdd9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephaven_server import Server\n",
    "s = Server(port=10000, jvm_args=[\"-Xmx40g\", \"-Dprocess.info.system-info.enabled=false\", \"-DAuthHandlers=io.deephaven.auth.AnonymousAuthenticationHandler\"])\n",
    "s.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f133e6-1369-4512-8e12-ec27a0eaae37",
   "metadata": {},
   "source": [
    "### Simulate streaming Amazon data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29cda8a-c967-4394-b6e7-25a69fe69255",
   "metadata": {},
   "source": [
    "First, we want to stream the Amazon review dataset in real-time. The Amazon dataset is static, so we will use [`TableReplayer`](https://deephaven.io/core/docs/how-to-guides/replay-data/) to simulate a real-time review stream. If you have a real-time review stream in a format like Kafka, you can directly use the stream without needing to simulate it.\n",
    "\n",
    "Start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0e072-1010-4c47-b99a-c110f475ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephaven import parquet, dtypes\n",
    "from deephaven.table import TableDefinition\n",
    "from deephaven.replay import TableReplayer\n",
    "from deephaven.time import to_j_instant\n",
    "\n",
    "from deephaven_ipywidgets import DeephavenWidget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26aba82-6ed5-477a-b482-917e53c2ee1c",
   "metadata": {},
   "source": [
    "Now, read the Amazon reviews into a Parquet table with [Deephaven's Parquet module](https://deephaven.io/core/docs/how-to-guides/data-import-export/parquet-import/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3656688-68b8-4551-9079-88080998c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table definition for review datasets\n",
    "reviews_def = TableDefinition({\n",
    "    \"rating\": dtypes.double,\n",
    "    \"title\": dtypes.string,\n",
    "    \"text\": dtypes.string,\n",
    "    \"parent_asin\": dtypes.string,\n",
    "    \"user_id\": dtypes.string,\n",
    "    \"timestamp\": dtypes.long\n",
    "})\n",
    "\n",
    "# read reviews into a single table\n",
    "reviews = parquet.read(\n",
    "    \"../amazon-data/reviews/\",\n",
    "    file_layout=parquet.ParquetFileLayout.FLAT_PARTITIONED,\n",
    "    table_definition=reviews_def\n",
    ")\n",
    "\n",
    "# convert timestamp to date-time timestamps\n",
    "reviews = (\n",
    "    reviews\n",
    "    .update(\"timestamp = epochMillisToInstant(timestamp)\")\n",
    "    .sort(\"timestamp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4ed2c-d928-41a6-abcd-40fd5bd3b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(DeephavenWidget(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6902027-cb15-476f-86f9-a39cc969d80e",
   "metadata": {},
   "source": [
    "The `reviews` table has 25.6 million observations spanning 9 months. Streaming through all of those observations in real time would take... 9 months. Instead, we randomly sample 1 in 10,000 reviews and replay that data at 10,000x speed. This emulates Amazon's real-world review frequency and lets us visualize long-term trends in just a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51642b0-cd4d-462f-891d-abd8f7d77a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum time from filtered table - faster to use UI than to compute with a query\n",
    "min_time = to_j_instant(\"2023-01-01T00:00:00.000Z\")\n",
    "\n",
    "# create replay start time and end time\n",
    "replay_start_time = to_j_instant(\"2024-01-01T00:00:00Z\")\n",
    "replay_end_time = to_j_instant(\"2024-01-01T00:36:00Z\")\n",
    "\n",
    "# replay data at 10,000x speed\n",
    "data_speed = 10_000\n",
    "\n",
    "# randomly sample data and create a timestamp that increments at 10,000x original speed\n",
    "reviews = (\n",
    "    reviews\n",
    "    .where(\"random() < 1 / data_speed\")\n",
    "    .update([\n",
    "        \"dist = (long)floor((timestamp - min_time) / data_speed)\",\n",
    "        \"replay_timestamp = replay_start_time + dist\"\n",
    "    ])\n",
    "    .drop_columns(\"dist\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e67604-9e28-4cbf-8bf5-5ac13558b007",
   "metadata": {},
   "source": [
    "Now, replay the data with Deephaven's [`TableReplayer`](https://deephaven.io/core/docs/how-to-guides/replay-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e58559-a467-40b0-81cb-a39b446801e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table replayer and start replay\n",
    "reviews_replayer = TableReplayer(replay_start_time, replay_end_time)\n",
    "reviews_ticking = reviews_replayer.add_table(reviews, \"replay_timestamp\").drop_columns(\"replay_timestamp\")\n",
    "reviews_replayer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a5117-dfcd-40d9-b252-b1a68541f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(DeephavenWidget(reviews_ticking))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b9d94-03bd-4b21-a3a7-9b98d50f71f0",
   "metadata": {},
   "source": [
    "### Real-time bot detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01fbba-959c-4e72-a77c-f2e791d608f6",
   "metadata": {},
   "source": [
    "With data flowing in simulation, it's possible to focus on the real-time detection of AI-bots. It's easier than you'd expect.\n",
    "\n",
    "First, load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937148e4-2c5c-402c-9992-a31a71144716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from deephaven.table_listener import listen, TableUpdate\n",
    "from deephaven.stream.table_publisher import table_publisher\n",
    "from deephaven.stream import blink_to_append_only\n",
    "from deephaven import new_table\n",
    "import deephaven.column as dhcol\n",
    "import deephaven.dtypes as dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74252f43-9958-4c5a-a4bc-27eb64a5d1df",
   "metadata": {},
   "source": [
    "Next, import the trained model's parameters into a new model object and load the tokenizer needed to transform the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf3f00-4dcf-4b39-9079-86f9e2f57986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress transformer parameter name warnings\n",
    "loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "for logger in loggers:\n",
    "    if \"transformers\" in logger.name.lower():\n",
    "        logger.setLevel(logging.ERROR)\n",
    "\n",
    "# instantiate model and load parameters\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.load_state_dict(torch.load(\"../detector/detector.pt\", weights_only=False))\n",
    "\n",
    "# get device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# instantiate tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10113726-3cb2-417b-abc1-63bbd6470c39",
   "metadata": {},
   "source": [
    "Now, we're going to walk through a real-time AI workflow step-by-step. The workflow looks like this:\n",
    "\n",
    "1. Create an object called a [`TablePublisher`](https://deephaven.io/core/docs/how-to-guides/table-publisher/) to publish new data to a ticking table. This table, `preds_blink`, will contain the new predictions.\n",
    "2. Define a function to perform inference and publish the results to `preds_blink`.\n",
    "3. Create a [`TableListener`](https://deephaven.io/core/docs/how-to-guides/table-listeners-python/) that will listen to the ticking data source and call the inference/publisher function as new data rolls in.\n",
    "4. Tie it all together by listening to the ticking source, performing inference on new inputs, and publishing the results to a new table.\n",
    "\n",
    "First, create the [`TablePublisher`](https://deephaven.io/core/docs/how-to-guides/table-publisher/) using the [`table_publisher`](https://deephaven.io/core/docs/reference/table-operations/create/TablePublisher/) function. This function returns an empty table to capture the published data, which we'll call `preds_blink`, and an object that publishes data to that table, which we'll call `preds_publish`. `preds_blink` is a [blink table](https://deephaven.io/core/docs/conceptual/table-types/#blink), meaning that it will only hold the most recent data from a given update cycle. Check out the [guide on table publishers](https://deephaven.io/core/docs/how-to-guides/table-publisher/) to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979509c-e51d-44a4-82b1-21293d636eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table publisher, and blink table that data will be published to\n",
    "preds_blink, preds_publish = table_publisher(\n",
    "    \"DetectorOutput\", {\n",
    "        \"rating\": dtypes.double,\n",
    "        \"parent_asin\": dtypes.string,\n",
    "        \"user_id\": dtypes.string,\n",
    "        \"timestamp\": dtypes.Instant,\n",
    "        \"gen_prob\": dtypes.float32\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f77333-10e3-4045-8944-5db11ced5e58",
   "metadata": {},
   "source": [
    "Next, define a function to perform the inference and publish the results to a new table using the table publisher defined previously. This function will be called every time more data rolls in, enabling Deephaven to perform real-time inference on only the most recent data. For simplicity, we've broken this into two functions: one to actually perform the inference on a given set of inputs, and one to call that function and publish the results to a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11481209-bfda-45f3-963b-c6dda0007b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that determines if a review was generated by a bot\n",
    "def detect_bot(text):\n",
    "    # tokenize text\n",
    "    tokenized_text = tokenizer(text.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # move input tensor to the same device as the model\n",
    "    tokenized_text = {key: value.to(device) for key, value in tokenized_text.items()}\n",
    "\n",
    "    # generate predictions using trained model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_text)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # the first column of logits corresponds to the negative class (non-AI-generated)\n",
    "    # and the second column corresponds to the positive class (AI-generated)\n",
    "    predictions = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# function to perform inference and publish the results to preds_blink\n",
    "def compute_and_publish_inference(inputs, features):\n",
    "\n",
    "    # get outputs from AI model\n",
    "    outputs = detect_bot(inputs)\n",
    "\n",
    "    # create new table with relevant features and outputs\n",
    "    output_table = new_table(\n",
    "        [\n",
    "            dhcol.double_col(\"rating\", features[\"rating\"]),\n",
    "            dhcol.string_col(\"parent_asin\", features[\"parent_asin\"]),\n",
    "            dhcol.string_col(\"user_id\", features[\"user_id\"]),\n",
    "            dhcol.datetime_col(\"timestamp\", features[\"timestamp\"]),\n",
    "            dhcol.float_col(\"gen_prob\", outputs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # publish inference to preds_blink\n",
    "    preds_publish.add(output_table)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f972ed-45ce-4424-8451-1348f6099d58",
   "metadata": {},
   "source": [
    "Next, we create a `TableListener` that listens to the ticking source and calls `compute_and_publish` on new data. To do this, define a function called `on_update` that takes two arguments, `update` and `is_replay`. Extract the added and modified data from the `update` argument using [`update.added()`](https://deephaven.io/core/pydoc/code/deephaven.table_listener.html#deephaven.table_listener.TableUpdate.added) and [`update.modified()`](https://deephaven.io/core/pydoc/code/deephaven.table_listener.html#deephaven.table_listener.TableUpdate.modified). See the [guide on table listeners](https://deephaven.io/core/docs/how-to-guides/table-listeners-python/) to learn more.\n",
    "\n",
    "Finally, we know that calling `compute_and_publish` will be expensive - neural network inference is not cheap. Instead of delaying the main thread with these expensive calculations, offload them to a separate thread using a [`ThreadPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor). This will collect the calculations to be done into a queue, and execute them as resources are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b4d1a-5e80-434d-a85a-1550d35c9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a ThreadPoolExecutor to multi-thread inference calculations\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "# function that the table listener will call as new reviews roll in\n",
    "def on_update(update: TableUpdate, is_replay: bool) -> None:\n",
    "    input_col = \"text\"\n",
    "    feature_cols = [\"rating\", \"parent_asin\", \"user_id\", \"timestamp\"]\n",
    "\n",
    "    # get table enries that were added or modified\n",
    "    adds = update.added(cols=[input_col, *feature_cols])\n",
    "    modifies = update.modified(cols=[input_col, *feature_cols])\n",
    "\n",
    "    # collect data from this cycle into objects to feed to inference and output\n",
    "    if adds and modifies:\n",
    "        inputs = np.hstack([adds[input_col], modifies[input_col]])\n",
    "        features = {feature_col: np.hstack([adds[feature_col], modifies[feature_col]]) for feature_col in feature_cols}\n",
    "    elif adds:\n",
    "        inputs = adds[input_col]\n",
    "        features = {feature_col: adds[feature_col] for feature_col in feature_cols}\n",
    "    elif modifies:\n",
    "        inputs = modifies[input_col]\n",
    "        features = {feature_col: modifies[feature_col] for feature_col in feature_cols}\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # submit inference work to ThreadPoolExecutor\n",
    "    executor.submit(compute_and_publish_inference, inputs, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e2b4f-9f46-4dce-a074-ccef3e3d11fc",
   "metadata": {},
   "source": [
    "Now, tie it all together. The [`listen`](https://deephaven.io/core/pydoc/code/deephaven.table_listener.html#deephaven.table_listener.listen) function below calls `on_update` every time a new review ticks into `reviews_ticking`. This runs the inference calculation on the new data and stors the result in `preds_blink`. Finally, [`blink_to_append_only`](https://deephaven.io/core/docs/reference/table-operations/create/blink-to-append-only/) converts `preds_blink` to an append-only table that stores the full history of the reviews and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075e01d-eb73-4db9-b98c-d91d1af4a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listen to ticking source and publish inference\n",
    "handle = listen(reviews_ticking, on_update)\n",
    "# convert preds_blink to a full-history table\n",
    "preds = blink_to_append_only(preds_blink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e6d49-cb24-49b1-89ef-2e44eceb583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(DeephavenWidget(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee80c49-cbca-47dd-83cf-de235d6d5b7a",
   "metadata": {},
   "source": [
    "The AI model output is captured in `preds` _in real time_ as data rolls into `reviews_ticking`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dh-amazon-venv)",
   "language": "python",
   "name": "dh-amazon-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
