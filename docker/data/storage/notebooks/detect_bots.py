import concurrent.futures
import logging
import torch
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification
from deephaven.table_listener import listen, TableUpdate
from deephaven.stream.table_publisher import table_publisher
from deephaven.stream import blink_to_append_only
from deephaven import new_table
import deephaven.column as dhcol
import deephaven.dtypes as dtypes

# suppress transformer parameter name warnings
loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]
for logger in loggers:
    if "transformers" in logger.name.lower():
        logger.setLevel(logging.ERROR)

# instantiate model and load parameters
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model.load_state_dict(torch.load("/detector/detector.pt", weights_only=False))

# get device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# instantiate tokenizer
tokenizer = BertTokenizer.from_pretrained(
    'bert-base-uncased',
    do_lower_case=True,
    padding=True,
    truncation=True,
    max_length=128,
    clean_up_tokenization_spaces=True
)

# create table publisher, and blink table that data will be published to
preds_blink, preds_publish = table_publisher(
    "DetectorOutput", {
        "rating": dtypes.double,
        "parent_asin": dtypes.string,
        "user_id": dtypes.string,
        "timestamp": dtypes.Instant,
        "gen_prob": dtypes.float32
    },
)

# function that determines if a review was generated by a bot
def detect_bot(text):
    # tokenize text
    tokenized_text = tokenizer(text.tolist(), padding=True, truncation=True, return_tensors='pt')

    # move input tensor to the same device as the model
    tokenized_text = {key: value.to(device) for key, value in tokenized_text.items()}

    # generate predictions using trained model
    with torch.no_grad():
        outputs = model(**tokenized_text)
        logits = outputs.logits

    # the first column of logits corresponds to the negative class (non-AI-generated) 
    # and the second column corresponds to the positive class (AI-generated)
    predictions = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()

    return predictions

# function to perform inference and publish the results to preds_blink
def compute_and_publish_inference(inputs, features):

    # get outputs from AI model
    outputs = detect_bot(inputs)

    # create new table with relevant features and outputs
    output_table = new_table(
        [
            dhcol.double_col("rating", features["rating"]),
            dhcol.string_col("parent_asin", features["parent_asin"]),
            dhcol.string_col("user_id", features["user_id"]),
            dhcol.datetime_col("timestamp", features["timestamp"]),
            dhcol.float_col("gen_prob", outputs)
        ]
    )

    # publish inference to preds_blink
    preds_publish.add(output_table)

    return None

# use a ThreadPoolExecutor to multi-thread inference calculations
executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)

# function that the table listener will call as new reviews roll in
def on_update(update: TableUpdate, is_replay: bool) -> None:
    input_col = "text"
    feature_cols = ["rating", "parent_asin", "user_id", "timestamp"]

    # get table enries that were added or modified
    adds = update.added(cols=[input_col, *feature_cols])
    modifies = update.modified(cols=[input_col, *feature_cols])

    # collect data from this cycle into objects to feed to inference and output
    if adds and modifies:
        inputs = np.hstack([adds[input_col], modifies[input_col]])
        features = {feature_col: np.hstack([adds[feature_col], modifies[feature_col]]) for feature_col in feature_cols}
    elif adds:
        inputs = adds[input_col]
        features = {feature_col: adds[feature_col] for feature_col in feature_cols}
    elif modifies:
        inputs = modifies[input_col]
        features = {feature_col: modifies[feature_col] for feature_col in feature_cols}
    else:
        return

    # submit inference work to ThreadPoolExecutor
    executor.submit(compute_and_publish_inference, inputs, features)

# load ticking reviews dataset
from stream_data import reviews_ticking    

# listen to ticking source and publish inference
handle = listen(reviews_ticking, on_update, do_replay=True)
# convert preds_blink to a full-history table
preds = blink_to_append_only(preds_blink)